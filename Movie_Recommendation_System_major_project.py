# -*- coding: utf-8 -*-
"""B21AI023_B21AI035_B21CS031_Major_Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kdcfzn9TF6YjQAJXCVEjdiJQnTObNhdY

# **Movie Recommendation System**

## **Using K-Means Algorithm**
"""

import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
from mpl_toolkits.axes_grid1 import make_axes_locatable
from sklearn.cluster import KMeans
from sklearn.metrics import mean_squared_error
import itertools
from sklearn.metrics import silhouette_samples, silhouette_score
from scipy.sparse import csr_matrix
# from pandas.api.types import SparseArray

movies = pd.read_csv("/content/movies.csv")
movies.head()

movies.info()

ratings = pd.read_csv("/content/ratings.csv")
ratings.head()

ratings.info()

ratings.shape

# Filtering data for only 4+ ratings
ratings = ratings[ratings['rating'] >= 4.2]

name_counts = ratings['movieId'].value_counts()
print(name_counts)
ratings = ratings[ratings['movieId'].isin(name_counts.index[name_counts > 10000])]

movies = movies[movies['movieId'].isin(name_counts.index[name_counts > 10000])]

movies.shape

ratings.shape

# randomly delete 80% of rows from df
rows_to_delete = ratings.sample(frac=0.8)
ratings = ratings.drop(rows_to_delete.index)

ratings.shape

movies.shape

tags = pd.read_csv("/content/tags.csv")
tags.head()

tags.info()

links = pd.read_csv("/content/links.csv")
links.head()

links.info()

dataset = pd.merge(movies, ratings, how ='inner', on ='movieId')
dataset.head()

print('The dataset contains: ', len(ratings), ' ratings of ', len(movies), ' movies.')
dataset.shape
dataset.nunique()

unique_user = ratings.userId.nunique(dropna = True)
unique_movie = ratings.movieId.nunique(dropna = True)
print("number of unique user:")
print(unique_user)
print("number of unique movies:")
print(unique_movie)

dataset = dataset.drop_duplicates()
print(dataset)

dataset.describe()

dataset.isnull()
dataset.isnull().sum()

x = dataset.genres
a = list()

for i in x:
 abc = i
 a.append(abc.split('|'))

a = pd.DataFrame(a)
b = a[0].unique()

for i in b:
 dataset[i] = 0

dataset.head(2000)

for i in b:
 dataset.loc[dataset['genres'].str.contains(i), i] = 1

dataset.head(2000)
dataset = dataset.drop(['genres','title'],axis =1)
dataset.head()

a=dataset
a=a.groupby('movieId')["rating"].mean()
a

sorted_ratings_wise_movie=a.sort_values(ascending=False)
sorted_ratings_wise_movie

def get_genre_ratings(ratings, movies, genres, column_names):
 genre_ratings = pd.DataFrame()

 for genre in genres:
  genre_movies = movies[movies['genres'].str.contains(genre) ]
  avg_genre_votes_per_user = ratings[ratings['movieId'].isin(genre_movies['movieId'])].loc[:,['userId', 'rating']].groupby(['userId'])['rating'].mean().round(2)

  genre_ratings = pd.concat([genre_ratings, avg_genre_votes_per_user], axis=1)

 print(genre_ratings)
 genre_ratings.columns = column_names
 return genre_ratings

genre_ratings = get_genre_ratings(ratings, movies, ['Romance', 'Sci-Fi', 'Comedy'],['avg_romance_rating', 'avg_scifi_rating', 'avg_comedy_rating'])
genre_ratings.head()

def bias_genre_rating_dataset(genre_ratings, score_limit_1, score_limit_2):
 biased_dataset = genre_ratings[((genre_ratings['avg_romance_rating'] < score_limit_1 - 0.2) & (genre_ratings['avg_scifi_rating'] > score_limit_2)) | ((genre_ratings['avg_scifi_rating'] < score_limit_1) & (genre_ratings['avg_romance_rating'] > score_limit_2))]
 biased_dataset = pd.concat([biased_dataset[:300], genre_ratings[:2]])
 biased_dataset = pd.DataFrame(biased_dataset.to_records())
 return biased_dataset

biased_dataset = bias_genre_rating_dataset(genre_ratings, 4.0, 3.5)
print( "Number of records: ", len(biased_dataset))
biased_dataset.head()

X = biased_dataset[['avg_scifi_rating','avg_romance_rating','avg_comedy_rating']].values
df = biased_dataset[['avg_scifi_rating','avg_romance_rating','avg_comedy_rating']]
possible_k_values = range(2, len(X)+1, 5)
# print(X)
print(len(X))

from sklearn.impute import SimpleImputer

# Create an imputer object with a suitable strategy (e.g., mean, median, etc.)
imputer = SimpleImputer(strategy='mean')

# Fit the imputer to the data and transform the data
X = imputer.fit_transform(X)

ratings_title = pd.merge(ratings, movies[['movieId', 'title']], on='movieId' )
user_movie_ratings = pd.pivot_table(ratings_title, index='userId', columns= 'title', values='rating')
print('dataset dimensions: ', user_movie_ratings.shape, '\n\nSubset example:')
user_movie_ratings.iloc[:6, :10]

def sort_by_rating_density(user_movie_ratings, n_movies, n_users):
 most_rated_movies = get_most_rated_movies(user_movie_ratings, n_movies)
 most_rated_movies = get_users_who_rate_the_most(most_rated_movies, n_users)
 return most_rated_movies

def get_most_rated_movies(user_movie_ratings, max_number_of_movies):
 # 1- Count
 print(user_movie_ratings.count())
 user_movie_ratings = user_movie_ratings.append(user_movie_ratings.count(), ignore_index=True)
 print(user_movie_ratings)
 # 2- sort
 user_movie_ratings_sorted = user_movie_ratings.sort_values(len(user_movie_ratings)-1, axis=1, ascending=False)
 user_movie_ratings_sorted = user_movie_ratings_sorted.drop(user_movie_ratings_sorted.tail(1).index)
 # 3- slice
 most_rated_movies = user_movie_ratings_sorted.iloc[:, :max_number_of_movies]
 return most_rated_movies

def get_users_who_rate_the_most(most_rated_movies, max_number_of_movies):
 # Get most voting users
 # 1- Count
 most_rated_movies['counts'] = pd.Series(most_rated_movies.count(axis=1))
 # 2- Sort
 most_rated_movies_users = most_rated_movies.sort_values('counts', ascending=False)
 # 3- Slice
 most_rated_movies_users_selection = most_rated_movies_users.iloc[:max_number_of_movies, :]
 most_rated_movies_users_selection = most_rated_movies_users_selection.drop(['counts'], axis=1)

 return most_rated_movies_users_selection

n_movies = 30
n_users = 18
most_rated_movies_users_selection = sort_by_rating_density(user_movie_ratings, n_movies, n_users)
print('dataset dimensions: ', most_rated_movies_users_selection.shape)
most_rated_movies_users_selection.head()

def draw_movies_heatmap(most_rated_movies_users_selection, axis_labels=True):

 # Reverse to match the order of the printed dataframe
 #most_rated_movies_users_selection = most_rated_movies_users_selection.iloc[::-1]

 fig = plt.figure(figsize=(15,4))
 ax = plt.gca()

# Draw heatmap
 heatmap = ax.imshow(most_rated_movies_users_selection, interpolation='nearest', vmin=0, vmax=5, aspect='auto')
 if axis_labels:
  ax.set_yticks(np.arange(most_rated_movies_users_selection.shape[0]) , minor=False)
  ax.set_xticks(np.arange(most_rated_movies_users_selection.shape[1]) , minor=False)
  ax.invert_yaxis()
  ax.xaxis.tick_top()
  labels = most_rated_movies_users_selection.columns.str[:40]
  ax.set_xticklabels(labels, minor=False)
  ax.set_yticklabels(most_rated_movies_users_selection.index, minor=False)
  plt.setp(ax.get_xticklabels(), rotation=90)

 else:
  ax.get_xaxis().set_visible(False)
  ax.get_yaxis().set_visible(False)

 ax.grid(False)
 ax.set_ylabel('User id')

 # Separate heatmap from color bar
 divider = make_axes_locatable(ax)
 cax = divider.append_axes("right", size="5%", pad=0.05)

 # Color bar
 cbar = fig.colorbar(heatmap, ticks=[5, 4, 3, 2, 1, 0], cax=cax)
 cbar.ax.set_yticklabels(['5 stars', '4 stars','3 stars','2 stars','1 stars','0 stars'])
 plt.show()

draw_movies_heatmap(most_rated_movies_users_selection)

user_movie_ratings = user_movie_ratings.sort_index()
most_rated_movies_1k = get_most_rated_movies(user_movie_ratings, 1000)

def sparse_clustering_errors(k, data):
 kmeans = KMeans(n_clusters=k).fit(data)
 predictions = kmeans.predict(data)
 cluster_centers = kmeans.cluster_centers_
 errors = [mean_squared_error(row, cluster_centers[cluster]) for row, cluster in zip(data, predictions)]
 return sum(errors)

sparse_ratings = csr_matrix(most_rated_movies_1k)

def draw_movie_clusters(clustered, max_users, max_movies):
 c=1
 for cluster_id in clustered.group.unique():
  # To improve visibility, we're showing at most max_users users and max_movies movies per cluster.
  # You can change these values to see more users & movies per cluster
  d = clustered[clustered.group == cluster_id].drop(['index', 'group'], axis=1)
  n_users_in_cluster = d.shape[0]

  d = sort_by_rating_density(d, max_movies, max_users)

  d = d.reindex(d.mean().sort_values(ascending=False).index, axis=1)
  d = d.reindex(d.count(axis=1).sort_values(ascending=False).index)
  d = d.iloc[:max_users, :max_movies]

  n_users_in_plot = d.shape[0]

  # We're only selecting to show clusters that have more than 9 users, otherwise, they're less interesting
  if len(d) > 9:
    print('cluster # {}'.format(cluster_id))
    print('# of users in cluster: {}.'.format(n_users_in_cluster), '# of users in plot: {}'.format(n_users_in_plot))
    fig = plt.figure(figsize=(15,4))
    ax = plt.gca()
    ax.invert_yaxis()
    ax.xaxis.tick_top()
    labels = d.columns.str[:40]
    ax.set_yticks(np.arange(d.shape[0]) , minor=False)
    ax.set_xticks(np.arange(d.shape[1]) , minor=False)
    ax.set_xticklabels(labels, minor=False)
    ax.get_yaxis().set_visible(False)

    # Heatmap
    heatmap = plt.imshow(d, vmin=0, vmax=5, aspect='auto')
    ax.set_xlabel('movies')
    ax.set_ylabel('User id')
    divider = make_axes_locatable(ax)
    cax = divider.append_axes("right", size="5%", pad=0.05)
    
    # Color bar
    cbar = fig.colorbar(heatmap, ticks=[5, 4, 3, 2, 1, 0], cax=cax)
    cbar.ax.set_yticklabels(['5 stars', '4 stars','3 stars','2 stars','1 stars','0 stars'])
    plt.setp(ax.get_xticklabels(), rotation=90, fontsize=9)
    plt.tick_params(axis='both', which='both', bottom='off', top='off', left='off', labelbottom='off', labelleft='off')
    #print('cluster # {} \n(Showing at most {} users and {} movies)'.format(cluster_id, max_users, max_movies))
    plt.show()

def bias_genre_rating_dataset(genre_ratings, score_limit_1, score_limit_2):
 biased_dataset = genre_ratings[((genre_ratings['avg_romance_rating'] < score_limit_1 - 0.2) & (genre_ratings['avg_scifi_rating'] > score_limit_2)) | ((genre_ratings['avg_scifi_rating'] < score_limit_1) & (genre_ratings['avg_romance_rating'] > score_limit_2))]
 biased_dataset = pd.concat([biased_dataset[:300], genre_ratings[:2]])
 biased_dataset = pd.DataFrame(biased_dataset.to_records())
 return biased_dataset

def sort_by_rating_density(user_movie_ratings, n_movies, n_users):
 most_rated_movies = get_most_rated_movies(user_movie_ratings, n_movies)
 most_rated_movies = get_users_who_rate_the_most(most_rated_movies, n_users)
 return most_rated_movies

import helper
import importlib
importlib.reload(helper)

# Create an imputer object with a suitable strategy (e.g., mean, median, etc.)
imputer = SimpleImputer(strategy='mean')

# Fit the imputer to the data and transform the data
sparse_ratings = imputer.fit_transform(sparse_ratings)

predictions = KMeans(n_clusters=20, algorithm='full').fit_predict(sparse_ratings)
max_users = 70
max_movies = 50
clustered = pd.concat([most_rated_movies_1k.reset_index(), pd.DataFrame({'group':predictions})], axis=1)
draw_movie_clusters(clustered, max_users, max_movies)

cluster_number = 4
n_users = 75
n_movies = 300
cluster = clustered[clustered.group == cluster_number].drop(['index', 'group'], axis=1)
cluster = sort_by_rating_density(cluster, n_movies, n_users)
draw_movies_heatmap(cluster, axis_labels=False)
cluster.fillna('').head()

movies

movie_name = ("Usual Suspects, The (1995)")
cluster[movie_name].mean()
cluster.mean().head(20)

ratings

user_id = 4
user_2_ratings = cluster.loc[user_id, :]
user_2_unrated_movies = user_2_ratings[user_2_ratings.isnull()]
avg_ratings = pd.concat([user_2_unrated_movies, cluster.mean()], axis=1, join='inner').loc[:,0]
avg_ratings.sort_values(ascending=False)[:20]

"""## **Using KNN Algorithm**"""

movies = movies.reset_index()
movies

movie_indices=pd.Series(movies.index, index=movies['movieId'])

ratings_title = pd.merge(ratings, movies[['movieId', 'title']], on='movieId' )
user_movie_ratings = pd.pivot_table(ratings_title, index='movieId', columns= 'userId', values='rating')
print('dataset dimensions: ', user_movie_ratings.shape, '\n\nSubset example:')
user_movie_ratings.iloc[:6, :10]

from sklearn.neighbors import NearestNeighbors

rating=csr_matrix(user_movie_ratings.values)

csr_data = csr_matrix(user_movie_ratings)

print(csr_data)

# Create an imputer object with a suitable strategy (e.g., mean, median, etc.)
imputer = SimpleImputer(strategy='mean')

# Fit the imputer to the data and transform the data
csr_data = imputer.fit_transform(csr_data)
rating = imputer.fit_transform(rating)

print(movie_indices)

knn= NearestNeighbors(metric='cosine', algorithm='brute', n_neighbors=20)
knn.fit(rating)
movie_list = movies[movies['title']=="Usual Suspects, The (1995)"]
print(len(movie_list))
if len(movie_list):
  movie_idx= movie_list.iloc[0]['movieId']
  movie_idx = movies[movies['movieId'] == movie_idx].index[0]
  print(movie_idx)
distances, indices=knn.kneighbors(csr_data[movie_idx], n_neighbors=20)
print(indices)
for i in indices[0]:
  if(i!=movie_idx):
    movie_title = movies.loc[movies.index==i, 'title']
    print(movie_title)
    # like.sort_index(axis=1, inplace=True)

"""## **Using Cosine Similarity Algorithm**"""

movies

ratings_df = ratings.copy()
ratings_df = ratings_df.drop('timestamp',1,inplace=True)

movies_df = movies.copy()

from sklearn.feature_extraction.text import CountVectorizer
from numpy.linalg import norm

#convert genres in list form
l_genres=[]
def to_list(x):
    global l_genres
    l=list(str(x).split('|'))

    for i in l:
        if i not in l_genres:
            l_genres.append(i)
    return l
def to_list_2(x):
    l=list(str(x).split(','))
    return l

# Function to convert all strings to lower case and strip names of spaces
def clean_data(x):
    if isinstance(x, list):
        return [str.lower(i.replace(" ", "")) for i in x]
    else:
        #Check if director exists. If not, return empty string
        if isinstance(x, str):
            return str.lower(x.replace(" ", ""))
        else:
            return ''


#Function that creates string mixture of preferences
def create_soup(x):
    return  ' '.join(x['genres'])

def find_genre(x,i):

    if i in x['genres']:
        return 1

#Function that adds score of first algorithm to dataframe
def set_score_1():

    # Get the similarity scores
    sim_scores= movies_df['similarity']
    print(sim_scores)

    # Sort the movies based on the similarity scores
    sim_scores = sim_scores.sort_values()
    # Get the movie indices
    # indices = [i[0] for i in sim_scores]

    # Add score_1 to movies
    movies_df['score_1']=0
    for i,sim in list(enumerate(sim_scores)):
        
        movies_df.loc[sim_indices[i],'score_1']=sim

#Function that adds score of first algorithm to dataframe

def run_algorithm_1(movieId):

    global sim_indices, movie_indices, cosine_sim

    #string mixture of user preferences
    user_soup='action adventure thriller sci-fi'
    
    #movies for first algorithm(considered only user preferred languages)
    print(movies_df)
    movies_1=movies_df
    movies_1=movies_1.reset_index()
    print(movies_1)
    movies_1.loc[len(movies_1),['soup']]=user_soup
    
    #counting the occurrences of words in the soup
    count = CountVectorizer()
    count.fit(movies_1['soup'])
    count_matrix = count.fit_transform(movies_1['soup'])
    count_array= count_matrix.toarray()
    print(count_array)
    
    print(count_matrix)
    for row in movies_1.index:
      A=count_array[-1]
      B=count_array[row]
      similarity=np.dot(A,B)/(norm(A)+norm(B))
      movies_df.loc[row, ['similarity']]=similarity
    print(movies_df)

    #indices to get movieIds
    sim_indices = pd.Series(movies_1['movieId'],index=movies_1.index)
    movie_indices=pd.Series(movies_df.index, index=movies_df['title'])

    # set_score_1()

#Funtion that returns overall recommended movies
def recommend(moviename,n):
    movie_list = movies_df[movies_df['title']==moviename]
    print(len(movie_list))
    if len(movie_list):
      movie_idx= movie_list.iloc[0]['movieId']
      print(movie_idx)

    #run algorithms
    run_algorithm_1(movie_idx)

    #Add scores of both algorithms
    movies_df['score']=movies_df['similarity']

    # Make the scores of already listened movies to zero
    # user_like=like[userId]
    # for i in range(len(user_like)):
        # if user_like[i]==1:
            # movies_df.loc[i,'score']=0
    
    #Sort the movies according to score
    movies_df.sort_values(by='similarity', inplace=True, ascending=False)
    print(movies_df)
    #return the recommended movies
    return movies_df['title'].iloc[1:n]


features = ['genres',]

for feature in features:
    movies_df[feature]=movies_df[feature].apply(to_list)
    movies_df[feature] = movies_df[feature].apply(clean_data)

movies_df['soup'] = movies_df.apply(create_soup, axis=1)

print(recommend("Usual Suspects, The (1995)",20))